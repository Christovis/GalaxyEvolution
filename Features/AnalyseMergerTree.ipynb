{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from readtree import DHaloReader as DHalo\n",
    "from readtree import read_tree_keys\n",
    "from SubHalos import SubHalos as SHfuncs\n",
    "from SubHalos import read_subhalo_keys\n",
    "from SubHalos import read_group_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SubFind\n",
    "#sh_orig_file = '/cosma6/data/dp004/dc-arno1/SZ_project/'\\\n",
    "#               'dark_matter_only/L62_N512_GR/'\n",
    "#sh_reor_file = '/cosma5/data/dp004/dc-beck3/Galaxy_Evolution/'\\\n",
    "#               'SubFind/dm_only/L62_N512_GR/subfind.0.hdf5'\n",
    "#mt_file = '/cosma5/data/dp004/dc-oles1/dhalo/out/trees/GR/'\\\n",
    "#          'treedir_075/tree_075.0.hdf5'  # 71 is 41, simply because\n",
    "\n",
    "sh_orig_file = \"/gpfs/data/Eagle/yanTestRuns/MergerTree/\" \\\n",
    "            \"Dec14/L0100N1504/EAGLE_L0100N1504_db.hdf5\"\n",
    "mt_file = \"/gpfs/data/Eagle/yanTestRuns/MergerTree/\" \\\n",
    "            \"Dec14/L0100N1504/EAGLE_L0100N1504_db.hdf5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Unable to open object (component not found)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-178b163bb6cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcolumns_2dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"/haloTrees/%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mcolumns_1dim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0moid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0motype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Unable to open object (component not found)'"
     ]
    }
   ],
   "source": [
    "filename = mt_file\n",
    "columns = [\n",
    "    \"nodeIndex\",\n",
    "    \"snapshotNumber\",\n",
    "    \"hostIndex\",\n",
    "    \"descendantHost\",\n",
    "    \"descendantIndex\",\n",
    "    \"isMainProgenitor\",\n",
    "    \"mainProgenitorIndex\",\n",
    "    \"isInterpolated\"\n",
    "]        \n",
    "\n",
    "with h5py.File(filename, \"r\") as data_file:\n",
    "\n",
    "    # Find dimensionality of keys\n",
    "    columns_1dim = [] \n",
    "    columns_2dim = [] \n",
    "    for column in columns:\n",
    "        if len(data_file[\"/haloTrees/%s\" % column].shape) == 1:\n",
    "            columns_1dim.append(column)\n",
    "        else:\n",
    "            columns_2dim.append(column)\n",
    "\n",
    "    # 1D keys\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            column: data_file[\"/haloTrees/%s\" % column].value\n",
    "            for column in columns_1dim\n",
    "        },\n",
    "        columns=columns_1dim\n",
    "    ).set_index(\"nodeIndex\")\n",
    "    del columns_1dim\n",
    "\n",
    "    # 2D keys\n",
    "    for column in columns_2dim:\n",
    "        if column == 'position':\n",
    "            pos = data_file[\"/haloTrees/%s\" % column].value\n",
    "            data['X'] = pd.Series(pos[:, 0], index=data.index)\n",
    "            data['Y'] = pd.Series(pos[:, 1], index=data.index)\n",
    "            data['Z'] = pd.Series(pos[:, 2], index=data.index)\n",
    "    del columns_2dim\n",
    "\n",
    "    ## eliminate fake elements with isIntegrated=1\n",
    "    #data = data[data.isInterpolated != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = mt_file\n",
    "columns = [\n",
    "    \"DhaloIndex\",\n",
    "    \"GroupIndex\",\n",
    "    \"SnapNum\",\n",
    "    \"DescendantID\",\n",
    "    \"HaloID\",\n",
    "    \"LastProgID\",\n",
    "    \"TopLeafID\"\n",
    "]\n",
    "\n",
    "data_file = h5py.File(filename, 'r')\n",
    "column_mt = []\n",
    "column_sh = []\n",
    "for column in columns:\n",
    "    if column in data_file['MergerTree']:\n",
    "        column_mt.append(column)\n",
    "    else:\n",
    "        column_sh.append(column)\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    {\n",
    "        column: data_file[\"/MergerTree/%s\" % column].value\n",
    "        for column in column_mt\n",
    "    },\n",
    "    columns=column_mt\n",
    ").set_index(\"HaloID\")\n",
    "#.set_index(data_file[\"/Merger/HaloID\"].value)\n",
    "\n",
    "for column in column_sh:\n",
    "    data[column] = pd.Series(data_file[\"/Subhalo/%s\" % column].value,\n",
    "                             index=data.index)\n",
    "data = data.rename(index=str,\n",
    "                   columns={\"SnapNum\": \"snapnum\", #\"HaloID\": \"nodeIndex\",\n",
    "                            \"DescendantID\" : \"descendantIndex\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy.version.version 1.14.5\n",
      "pandas.version.version 0.21.0\n",
      "redshift: 71\n",
      "nodeID_prog_desc 0.9775124947873552\n",
      "progcounts 0.9976456886673721 0\n",
      "redshift: 70\n",
      "get_indexer [  8003   8877   8004 ... 148417   1965  58412]\n",
      "progcounts 0.9970320649689745 1\n",
      "redshift: 69\n",
      "get_indexer [  8325   8326   8327 ... 158770 158775   4096]\n",
      "progcounts 0.9775088130138316 2\n",
      "redshift: 68\n",
      "get_indexer [  8608   8609   8611 ... 162314   1559   2044]\n",
      "progcounts 0.9813971835924536 3\n",
      "progcounts [[0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('numpy.version.version', np.version.version)\n",
    "print('pandas.version.version', pd.__version__)\n",
    "from itertools import chain\n",
    "\n",
    "snapcount = 0\n",
    "z2 = 26\n",
    "z1 = 23        \n",
    "for ss in range(z2, z1, -1):\n",
    "\n",
    "    if ss == z2:\n",
    "        _indx = np.where(data.snapshotNumber.values == ss)\n",
    "        nodeID = data.index.values[_indx]\n",
    "        _indx = np.where(data.snapshotNumber.values == ss-1)\n",
    "        nodeID_prog = data.index.values[_indx]\n",
    "        nodeID_prog_desc = data.descendantIndex.values[_indx]\n",
    "\n",
    "        # Initiliaze Array\n",
    "        progcounts = np.zeros((len(nodeID), z2-z1+1))\n",
    "        nodeID_obs_ref = [None]*len(nodeID)\n",
    "\n",
    "        # nodeID_prog_desc_unic is sorted\n",
    "        nodeID_prog_desc_unic, count = np.unique(nodeID_prog_desc,\n",
    "                                                 return_counts=True)\n",
    "        # remove -1's\n",
    "        nodeID_prog_desc_unic=nodeID_prog_desc_unic[1:]; count=count[1:]\n",
    "\n",
    "        # Nr. of progenitors for sub-&halos at snapshot z2\n",
    "        _indx_now = np.arange(nodeID.shape[0])[np.in1d(\n",
    "            nodeID, nodeID_prog_desc_unic,\n",
    "            assume_unique=True)]\n",
    "        now_sort_indx = np.argsort(nodeID[_indx_now])\n",
    "        pro_sort_indx = np.argsort(nodeID_prog_desc_unic)\n",
    "        progcounts[_indx_now[now_sort_indx], snapcount] = count[pro_sort_indx]\n",
    "            \n",
    "        # sort nodeID_prog to nodeID\n",
    "        df_prog = pd.DataFrame({'nodeID' : nodeID_prog,\n",
    "                                'nodeID_target' : nodeID_prog_desc})\n",
    "        print('nodeID_prog_desc', len(np.unique(nodeID_prog_desc))/len(nodeID_prog_desc))\n",
    "\n",
    "        nodeID_target = nodeID_prog_desc\n",
    "        df_target = pd.DataFrame({'nodeID':nodeID})\n",
    "    else:\n",
    "        df_now = df_prog\n",
    "        _indx = np.where(data.snapshotNumber.values == ss-1)\n",
    "        df_prog = pd.DataFrame({'nodeID' : data.index.values[_indx]})\n",
    "        nodeID_prog_desc = data.descendantIndex.values[_indx]\n",
    " \n",
    "        progcounts_local = np.zeros(df_now['nodeID'].size)\n",
    "        nodeID_prog_desc_unic, count = np.unique(nodeID_prog_desc,\n",
    "                                                 return_counts=True)\n",
    "        # remove -1's\n",
    "        nodeID_prog_desc_unic=nodeID_prog_desc_unic[1:]; count=count[1:]\n",
    "        \n",
    "        s = pd.Index(df_now['nodeID'].tolist())\n",
    "        _indx_now = s.get_indexer(list(nodeID_prog_desc_unic))\n",
    "        now_sort_indx = np.argsort(df_now['nodeID'].values[_indx_now])\n",
    "        pro_sort_indx = np.argsort(nodeID_prog_desc_unic)\n",
    "        progcounts_local[_indx_now[now_sort_indx]] = count[pro_sort_indx]\n",
    "        df_now['progcount'] = pd.Series(progcounts_local,\n",
    "                                        index=df_now.index, dtype=int)\n",
    "\n",
    "        # Nr. of progenitors for sub-&halos at snapshot z2\n",
    "        df_inter = df_now.groupby(['nodeID_target'], as_index=False)['progcount'].sum()\n",
    "        df_inter = df_inter[df_inter['nodeID_target'] >= 1e11]  # only real progeniteurs\n",
    "        df_inter = df_inter.drop_duplicates(subset=['nodeID_target'], keep='first')\n",
    "        \n",
    "        s = pd.Index(df_target['nodeID'].tolist())\n",
    "        _indx_now = s.get_indexer(df_inter['nodeID_target'].tolist())\n",
    "        now_sort_indx = np.argsort(df_target['nodeID'].values[_indx_now])\n",
    "        pro_sort_indx = np.argsort(df_inter['nodeID_target'].values)\n",
    "        progcounts[_indx_now[now_sort_indx], snapcount] = df_inter['progcount'].values[pro_sort_indx]\n",
    "\n",
    "        # sort nodeID_prog to nodeID\n",
    "        #s = pd.Index(df_now['nodeID'].tolist())\n",
    "        #_indx_now = s.get_indexer(list(nodeID_prog_desc_unic))\n",
    "        #df_now['nodeID_target'].values[_indx_now]\n",
    "        \n",
    "        obs_ref_local = np.zeros(df_prog['nodeID'].size)\n",
    "        for ii in range(len(nodeID_prog_desc_unic)):\n",
    "            tarID = df_now.loc[df_now['nodeID'] == nodeID_prog_desc_unic[ii], 'nodeID_target']\n",
    "            _indx = np.where(nodeID_prog_desc == nodeID_prog_desc_unic[ii])\n",
    "            obs_ref_local[_indx] = int(tarID)\n",
    "        df_prog['nodeID_target'] = pd.Series(obs_ref_local,\n",
    "                                             index=df_prog.index)\n",
    "\n",
    "    print('progcounts', np.mean(progcounts[:, snapcount]), snapcount)\n",
    "    snapcount += 1\n",
    "del nodeID_prog, nodeID_prog_desc\n",
    "del df_target, df_now, df_inter, df_prog\n",
    "print('progcounts', progcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #_indx_now = np.arange(df_target['nodeID'].size)[np.isin(df_target['nodeID'].values,\n",
    "        #                                                        df_inter['nodeID_target'].values,\n",
    "        #                                                        assume_unique=False)]\n",
    "        #_indx_prog = np.arange(df_inter['nodeID_target'].size)[np.isin(df_inter['nodeID_target'].values,\n",
    "        #                                                               df_target['nodeID'].values,\n",
    "        #                                                               assume_unique=False)]\n",
    "        #_indx_now = dznter['nodeID_target'].tolist())].index        \n",
    "        #_indx_now = []\n",
    "        #for ii in range(df_inter['nodeID_target'].size):\n",
    "        #    _indx = np.where(df_target['nodeID'].values == df_inter['nodeID_target'].values[ii])\n",
    "        #    _indx_now.append(_indx[0])\n",
    "        #_indx_now = np.unique(np.fromiter(chain.from_iterable(_indx_now), dtype='int'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
